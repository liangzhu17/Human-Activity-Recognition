# -*- coding: utf-8 -*-
"""oversampling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pQJ6w_luaVYdIajLncfNe5FehnwAwvSt
"""

# reading above csv files as numpy arrays and check dimmensions

my_train_d = pd.read_csv('/content/drive/My Drive/Masterlab/csv files/train_x.csv',skiprows=1,usecols=range(1,7),sep=',',header=None)
my_train_l = pd.read_csv('/content/drive/My Drive/Masterlab/csv files/train_y.csv',skiprows=1,usecols=range(1,13),sep=',',header=None)
my_val_d = pd.read_csv('/content/drive/My Drive/Masterlab/csv files/val_x.csv',skiprows=1,usecols=range(1,7),sep=',',header=None)
my_val_l = pd.read_csv('/content/drive/My Drive/Masterlab/csv files/val_y.csv',skiprows=1,usecols=range(1,13),sep=',',header=None)
my_test_d = pd.read_csv('/content/drive/My Drive/Masterlab/csv files/test_x.csv',skiprows=1,usecols=range(1,7),sep=',',header=None)
my_test_l = pd.read_csv('/content/drive/My Drive/Masterlab/csv files/test_y.csv',skiprows=1,usecols=range(1,13),sep=',',header=None)

print(np.array(my_train_d).shape) 
print(np.array(my_train_l).shape)  # (469194, 12)
print(np.array(my_val_d).shape) 
print(np.array(my_val_l).shape)    # (133831, 12)
print(np.array(my_test_d).shape)   # (69910, 6)
print(np.array(my_test_l).shape)   # (69910, 12)

from collections import Counter

f= lambda x: x+1
y = f(np.argmax(train_y, axis=1))
counter = Counter(y)
print(counter)

# random oversampling
from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=0)


x_resampled, y_resampled = ros.fit_sample(np.array(train_x), np.array(train_y))
c = np.argmax(y_resampled, axis=1)
c = Counter(c)
print(c)

window_size = 250
window_shift = 125
batch_size = 100
epoch = 10

train_x = tf.convert_to_tensor(my_train_d.values)
train_y = tf.convert_to_tensor(my_train_l.values, dtype=tf.int16)
val_x = tf.convert_to_tensor(my_val_d.values)
val_y = tf.convert_to_tensor(my_val_l.values, dtype=tf.int16)
test_x = tf.convert_to_tensor(my_test_d.values)
test_y = tf.convert_to_tensor(my_test_l.values, dtype=tf.int16)


def sliding_win(x, y, window_size, window_shift, batch_size):
    # np.argmax(y)  # from one-hot translate back
    ds_x = tf.data.Dataset.from_tensor_slices(x)
    ds_y = tf.data.Dataset.from_tensor_slices(y)
    # ds_y = ds_y.map(tf.one_hot())
    ds_x = ds_x.window(size=window_size, shift=window_shift, drop_remainder=True).flat_map(
        lambda x: x.batch(window_size)).batch(batch_size, drop_remainder=True)
    ds_y = ds_y.window(size=window_size, shift=window_shift, drop_remainder=True).flat_map(
        lambda y: y.batch(window_size)).batch(batch_size, drop_remainder=True)
    ds = tf.data.Dataset.zip((ds_x, ds_y))
    return ds

def parse_func(x,y):
  return sequence, label

# create new dataset after oversampling

x_resampled = tf.convert_to_tensor(x_resampled)
y_resampled = tf.convert_to_tensor(y_resampled, dtype=tf.int16)

train_sw = sliding_win(train_x, train_y, window_size, window_shift, batch_size)
train_resampled_sw = sliding_win(x_resampled, y_resampled, window_size, window_shift, batch_size)
test_sw = sliding_win(test_x,test_y, window_size, window_shift, batch_size)
val_sw = sliding_win(val_x,val_y, window_size, window_shift, batch_size)
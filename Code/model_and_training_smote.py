# -*- coding: utf-8 -*-
"""model_and_training_SMOTE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ocJyB4PgRgJOe8_yCJGt-9oo1WGrLSSs
"""

!pip3 install tensorflow-gpu==2.0.0-beta0

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
import os
import glob
import numpy as np
import numpy.random as npr
import pandas as pd
import tensorflow.keras as k
import tensorflow.keras.layers as l
from tensorflow.keras.layers import Dense, Flatten, Conv2D
from tensorflow.keras import Model
import matplotlib.pyplot as plt
import random
from keras import optimizers
import pandas as pd

# reading above csv files as numpy arrays and check dimmensions

my_train_d = pd.read_csv('/content/drive/My Drive/Masterlab/csv files/train_x.csv',skiprows=1,usecols=range(1,7),sep=',',header=None)
my_train_l = pd.read_csv('/content/drive/My Drive/Masterlab/csv files/train_y.csv',skiprows=1,usecols=range(1,13),sep=',',header=None)
my_val_d = pd.read_csv('/content/drive/My Drive/Masterlab/csv files/val_x.csv',skiprows=1,usecols=range(1,7),sep=',',header=None)
my_val_l = pd.read_csv('/content/drive/My Drive/Masterlab/csv files/val_y.csv',skiprows=1,usecols=range(1,13),sep=',',header=None)
my_test_d = pd.read_csv('/content/drive/My Drive/Masterlab/csv files/test_x.csv',skiprows=1,usecols=range(1,7),sep=',',header=None)
my_test_l = pd.read_csv('/content/drive/My Drive/Masterlab/csv files/test_y.csv',skiprows=1,usecols=range(1,13),sep=',',header=None)


print(np.array(my_train_d).shape) 
print(np.array(my_train_l).shape)  # (469194, 12)
print(np.array(my_val_d).shape) 
print(np.array(my_val_l).shape)    # (133831, 12)
print(np.array(my_test_d).shape)   # (69910, 6)
print(np.array(my_test_l).shape)   # (69910, 12)

from collections import Counter

f= lambda x: x+1
y = f(np.argmax(train_y, axis=1))
counter = Counter(y)
print(counter)

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=0)


x_resampled, y_resampled = ros.fit_sample(np.array(train_x), np.array(train_y))
c = np.argmax(y_resampled, axis=1)
c = Counter(c)
print(c)

# use smote to oversample

from imblearn.over_sampling import SMOTE

# smote = SMOTE(ratio='minority')
#smote = SMOTE(random_state=0)
smote = SMOTE()
x_sm, y_sm = smote.fit_sample(np.array(train_x), np.array(train_y))


c = f(np.argmax(y_sm, axis=1))
c = Counter(c)
print(c)

window_size = 250
window_shift = 125
batch_size = 32
epoch = 15

train_x = tf.convert_to_tensor(my_train_d.values)
train_y = tf.convert_to_tensor(my_train_l.values, dtype=tf.int16)
val_x = tf.convert_to_tensor(my_val_d.values)
val_y = tf.convert_to_tensor(my_val_l.values, dtype=tf.int16)
test_x = tf.convert_to_tensor(my_test_d.values)
test_y = tf.convert_to_tensor(my_test_l.values, dtype=tf.int16)


def sliding_win(x, y, window_size, window_shift, batch_size):

    ds_x = tf.data.Dataset.from_tensor_slices(x)
    ds_y = tf.data.Dataset.from_tensor_slices(y)
    # ds_y = ds_y.map(tf.one_hot())
    ds_x = ds_x.window(size=window_size, shift=window_shift, drop_remainder=True).flat_map(
        lambda x: x.batch(window_size)).batch(batch_size, drop_remainder=True)
    ds_y = ds_y.window(size=window_size, shift=window_shift, drop_remainder=True).flat_map(
        lambda y: y.batch(window_size)).batch(batch_size, drop_remainder=True)
    ds = tf.data.Dataset.zip((ds_x, ds_y))
    return ds

train_sw = sliding_win(train_x, train_y, window_size, window_shift, batch_size)
train_resampled_sw = sliding_win(x_sm, y_sm, window_size, window_shift, batch_size)
test_sw = sliding_win(test_x,test_y, window_size, window_shift, batch_size)
val_sw = sliding_win(val_x,val_y, window_size, window_shift, batch_size)

# build model 
# windows 500,250 optional

def base_model():
    inputs = tf.keras.layers.Input(shape=(window_size, 6))

    output = tf.keras.layers.LSTM(units=128, return_sequences=True)(inputs)

    output = tf.keras.layers.Dense(units=64, activation='relu')(output)

    output = tf.keras.layers.Dropout(rate=0.25)(output)

    output = tf.keras.layers.Dense(units=12, activation='softmax')(output)
    model = tf.keras.models.Model(inputs=inputs, outputs=output)
    return model


"""def base_model():
    inputs = tf.keras.layers.Input(shape=(window_size, 6))

    output = tf.keras.layers.LSTM(units=128, return_state=True)(inputs)
    
    return model"""
    
model = base_model()
# model.load_weights("checkpoint_90_test/cp.ckpt")
model.summary()
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

checkpoint_path = "test_ckpt/cp.ckpt"

# checkpoint_dir = os.path.dirname(checkpoint_path)
# Create a callback that saves the model's weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)
model.fit(train_resampled_sw, epochs=epoch, validation_data=val_sw, callbacks=[cp_callback])
model.save('my_model.h5')
model.save_weights('my_model_weights.h5')


# model.save('my_model.h5',save_format="tf")

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
f = lambda x : x + 1

# test accuracy
def check_testing():
  l_arr = []
  p_arr = []
    
  for d,l in test_sw:  
                                          # after unbatch   d(500,6)   l(500,12)
    if len(l_arr) == 0 and len(p_arr)==0:
      pred = model.predict(d)
      p = pred.reshape(-1,12)      # reduce dimension
      l = tf.reshape(l,(-1,12))    
      p_reformed = f(np.argmax(p, axis=1))       #(40000,)
      l_reformed = f(np.argmax(l, axis=1))       #(40000,)
      l_arr = l_reformed
      p_arr = p_reformed
    else:
      pred = model.predict(d)
      p = pred.reshape(-1,12)      # reduce dimension
      l = tf.reshape(l,(-1,12))
      p_reformed = f(np.argmax(p, axis=1))      
      l_reformed = f(np.argmax(l, axis=1))

      l_arr = np.hstack((l_arr, l_reformed))
      p_arr = np.hstack((p_arr, p_reformed))      #last (200000,)

  acc_score = accuracy_score(l_arr, p_arr)
  recall = recall_score(l_arr, p_arr, average="macro")
  precision = precision_score(l_arr, p_arr, average="macro")
  f1 = f1_score(l_arr, p_arr, average="macro")

  return [acc_score, recall, precision, f1]

print(check_testing())


def get_l_p():

  l_arr = []
  p_arr = []
    
  for d,l in test_sw:  
                                          # after unbatch   d(500,6)   l(500,12)
    if len(l_arr) == 0 and len(p_arr)==0:
      pred = model.predict(d)
      p = pred.reshape(-1,12)      # reduce dimension
      l = tf.reshape(l,(-1,12))    
      p_reformed = f(np.argmax(p, axis=1))       #(40000,)
      l_reformed = f(np.argmax(l, axis=1))       #(40000,)
      l_arr = l_reformed
      p_arr = p_reformed
    else:
      pred = model.predict(d)
      p = pred.reshape(-1,12)      # reduce dimension
      l = tf.reshape(l,(-1,12))
      p_reformed = f(np.argmax(p, axis=1))      
      l_reformed = f(np.argmax(l, axis=1))

      l_arr = np.hstack((l_arr, l_reformed))
      p_arr = np.hstack((p_arr, p_reformed))      #last (200000,)


  return l_arr, p_arr

# Metrics confusion matrix

def plot_confusion_matrix(cm, savename, title='Confusion Matrix'):
    plt.figure(figsize=(12, 8), dpi=100)
    np.set_printoptions(precision=2)

    # the probabilities in confusion matrix
    ind_array = np.arange(len(classes))
    x, y = np.meshgrid(ind_array, ind_array)
    for x_val, y_val in zip(x.flatten(), y.flatten()):
        c = cm[y_val][x_val]
        if c > 0.001:
            plt.text(x_val, y_val, "%0.2f" % (c,), color='red', fontsize=5, va='center', ha='center')

    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.binary)
    plt.title(title)
    plt.colorbar()
    xlocations = np.array(range(len(classes)))
    plt.xticks(xlocations, classes, rotation=90)
    plt.yticks(xlocations, classes)
    plt.ylabel('Actual label')
    plt.xlabel('Predict label')

    # offset the tick
    tick_marks = np.array(range(len(classes))) + 0.5
    plt.gca().set_xticks(tick_marks, minor=True)
    plt.gca().set_yticks(tick_marks, minor=True)
    plt.gca().xaxis.set_ticks_position('none')
    plt.gca().yaxis.set_ticks_position('none')
    plt.grid(True, which='minor', linestyle='-')
    plt.gcf().subplots_adjust(bottom=0.15)

    # show confusion matrix
    plt.savefig(savename, format='png')
    plt.show()


classes = ['WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS', 'SITTING', 'STANDING', 'LAYING', 'STAND_TO_SIT',
           'SIT_TO_STAND', 'SIT_TO_LIE', 'LIE_TO_SIT', 'STAND_TO_SIT', 'LIE_TO_STAND']

"""random_numbers = np.random.randint(6, size=50) [4 3 4 4 1 3 5 4 0 5 1 1 2 2 1 2 3 1 2 4 4 0 4 1 3 0 0 1 4 4 4 3 1 2 1 3 4
 4 0 2 0 5 5 4 2 0 3 2 1 2]
y_t = random_numbers.copy() 
random_numbers[:10] = np.random.randint(6, size=10)  # 将前10个样本的值进行随机更改 [0 4 5 1 0 5 0 2 0 1 1 1 2 2 1 2 3 1 2 4 4 0 4 1 3 0 0 1 4 4 4 3 1 2 1 3 4
 4 0 2 0 5 5 4 2 0 3 2 1 2]
y_p = random_numbers"""

y_true, y_pred = get_l_p()
print(np.array(y_true).shape)
print(np.array(y_pred).shape)
# 获取混淆矩阵
cm = confusion_matrix(y_true, y_pred)
plot_confusion_matrix(cm, 'confusion_matrix.png', title='confusion matrix')
"""if __name__ == '__main__':
    print()"""



